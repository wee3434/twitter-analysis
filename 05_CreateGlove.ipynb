{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we're going to generate our own glove based on our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(sent, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "#    for sent in texts:\n",
    "    doc = nlp(\" \".join(sent)) \n",
    "    texts_out.extend([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    return [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf = pd.read_csv(\"bDMARD without URL for Glove.csv\")\n",
    "cdf = pd.read_csv(\"cDMARD without URL for Glove.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentList = []\n",
    "\n",
    "for tweet in cdf.tweet:\n",
    "    sentList.append(lemmatization(remove_stopwords(tweet)))\n",
    "for tweet in bdf.tweet:\n",
    "    sentList.append(lemmatization(remove_stopwords(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 40 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n",
      "Epoch 30\n",
      "Epoch 31\n",
      "Epoch 32\n",
      "Epoch 33\n",
      "Epoch 34\n",
      "Epoch 35\n",
      "Epoch 36\n",
      "Epoch 37\n",
      "Epoch 38\n",
      "Epoch 39\n"
     ]
    }
   ],
   "source": [
    "#importing the glove library\n",
    "#credit to: https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b\n",
    "from glove import Corpus, Glove\n",
    "# creating a corpus object\n",
    "corpus = Corpus() \n",
    "#training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "corpus.fit(sentList, window=10)\n",
    "#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
    "#We can set the learning rate as it uses Gradient Descent and number of components\n",
    "glove = Glove(no_components=300, learning_rate=0.05)\n",
    " \n",
    "glove.fit(corpus.matrix, epochs=40, no_threads=4, verbose=True)\n",
    "\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import  Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_index(loc):\n",
    "    f = open(loc,encoding=\"utf8\")\n",
    "    embeddings_index = {}\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "#creates word embedding matrix\n",
    "def create_emb():\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_dict.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "with io.open(\"myGlove300.txt\",  \"w\", encoding=\"utf-8\") as f:\n",
    "    for key, val in glove.dictionary.items():\n",
    "        f.write(key+ \" \"+ re.sub(\"(\\\\n|\\[|\\])\", \"\",np.array2string(glove.word_vectors[val])) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
